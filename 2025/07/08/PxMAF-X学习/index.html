

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="牧丛">
  <meta name="keywords" content="">
  
    <meta name="description" content="这是一个关于3D人体网格恢复的学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="PxMAF-X学习">
<meta property="og:url" content="http://example.com/2025/07/08/PxMAF-X%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="MYCDherder&#39;s Blog">
<meta property="og:description" content="这是一个关于3D人体网格恢复的学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/cover/cover6.jpg">
<meta property="article:published_time" content="2025-07-07T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-08T07:34:31.343Z">
<meta property="article:author" content="MYCDherder">
<meta property="article:tag" content="SMPL">
<meta property="article:tag" content="PxMAF-X">
<meta property="article:tag" content="3D人体模型恢复">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/cover/cover6.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>PxMAF-X学习 - MYCDherder&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>MYCDherder&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="PxMAF-X学习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        牧丛
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-08 00:00" pubdate>
          2025年7月8日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          66 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">PxMAF-X学习</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="pxmaf-x学习">PxMAF-X学习</h1>
<p>本篇较为复杂，笔者也是边学边写，所以可能不能形成总体—&gt;细节拆分的布局，只能先暂时是分点—&gt;总结的学习步骤</p>
<h2 id="瓶颈块">瓶颈块</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.Module):<br>    expansion = <span class="hljs-number">4</span> <span class="hljs-comment"># 输出通道数是中间通道数的4倍</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># inplanes:输入通道数</span><br>        <span class="hljs-comment"># planes:压缩过后的中间层通道数</span><br>        <span class="hljs-comment"># stride:步长，控制特征图尺寸</span><br>        <span class="hljs-comment"># downsample:下采样（当输入和输出尺寸不匹配时使用）</span><br>        <span class="hljs-built_in">super</span>(Bottleneck, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 1x1卷积:压缩</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(planes)<br>        <span class="hljs-comment"># 3x3卷积:处理</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="hljs-number">3</span>, stride=stride,<br>                               padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn2 = nn.BatchNorm2d(planes)<br>        <span class="hljs-comment"># 1x1卷积:解压</span><br>        <span class="hljs-variable language_">self</span>.conv3 = nn.Conv2d(planes, planes * <span class="hljs-number">4</span>, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn3 = nn.BatchNorm2d(planes * <span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 激活函数</span><br>        <span class="hljs-variable language_">self</span>.downsample = downsample<br>        <span class="hljs-variable language_">self</span>.stride = stride<br><br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>   		residual = x  <span class="hljs-comment"># 保存原始输入（捷径）</span><br><br>    	<span class="hljs-comment"># 主路径：三层卷积</span><br>    	out = <span class="hljs-variable language_">self</span>.conv1(x)<br>    	out = <span class="hljs-variable language_">self</span>.bn1(out)<br>    	out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>    	out = <span class="hljs-variable language_">self</span>.conv2(out)<br>    	out = <span class="hljs-variable language_">self</span>.bn2(out)<br>    	out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>    	out = <span class="hljs-variable language_">self</span>.conv3(out)<br>    	out = <span class="hljs-variable language_">self</span>.bn3(out)<br><br>    	<span class="hljs-comment"># 如果需要下采样（输入和输出尺寸不匹配），对原始输入也做下采样</span><br>    	<span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    	    residual = <span class="hljs-variable language_">self</span>.downsample(x)<br><br>    	out += residual  <span class="hljs-comment"># 主路径输出+原始输入（残差连接）</span><br>    	out = <span class="hljs-variable language_">self</span>.relu(out)  <span class="hljs-comment"># 最后再通过激活函数</span><br><br>    	<span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<p>这段定义了一个类定义的是ResNet中的瓶颈块，ResNet是残差网络，其主要的作用就是在神经网络传递的过程中解决退化问题，神经网络在不断的传递过程中，信息会存在偏差或缺失，这就导致深层次的神经网络可能会比浅层次的网络效果更差，而ResNet就是为了解决这个问题，主要得解决办法就是“抄近路”，每几层就加上一条“捷径”，让信息跳过中间层直接传递，这样就保证了信息不会丢失，而其中的基本单元就是<strong>残差块</strong>，残差快包括两个部分，分别是主路径和捷径，而我们这边定义的<strong>瓶颈块</strong>就是一种特殊的残差块，它是通过三个卷积层的组合，来完成一个压缩
- 处理 - 解压的过程。我在代码上加了一些注释，应该可以帮助理解。</p>
<p>举一个实例，就是这个样子</p>
<p>假设输入是256通道的特征图，中间层通道数设为64：</p>
<ol type="1">
<li><p>压缩阶段（1x1 卷积）：</p>
<p>输入：256 通道 → <code>conv1</code> → 64 通道</p>
<p>减少了 3/4 的通道数，降低计算量</p></li>
<li><p>处理阶段（3x3 卷积）：</p>
<p>64 通道 → <code>conv2</code> → 64 通道</p>
<p>保持通道数不变，专注提取特征</p></li>
<li><p>解压阶段（1x1 卷积）：</p>
<p>64 通道 → <code>conv3</code> → 256 通道（恢复原始通道数）</p></li>
<li><p>残差连接：</p>
<p>将原始输入（256 通道）直接加到处理后的输出上</p>
<p>公式：<code>最终输出 = 卷积处理结果 + 原始输入</code></p></li>
</ol>
<h3 id="conv2d函数的意思">Conv2d函数的意思</h3>
<p>代码中有这样一行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.Conv2d(inplanes, planes, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>
<p><code>Conv2d</code>是什么呢？这行代码表示的是一个1x1卷积层，它在这里负责两件事，第一个是把高通道数压成低通道数，第二个是再把低通道数恢复到高通道数，我们用最通俗的话来说，就是如果你是一个乐队演奏家，要调整现在的乐器的音量：你原先有10个乐器（通道）同时演奏，现在你对它们进行1x1卷积，给每个乐器提供一个“音量调整系数”，在调整后，就可以只用保持5个乐器的声音（通道数减少），或者达到20个乐器的效果（通道数增加）</p>
<h3 id="batchnorm2d函数的意思">BatchNorm2d函数的意思</h3>
<p>还有这样的一行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(planes)<br></code></pre></td></tr></table></figure>
<p><code>BatchNorm2d</code>又是什么呢？这里是使用了PyTorch创建了一个<strong>二维批量归一化层（Batch
Normalization）</strong>，简称<strong>BN
层</strong>，这个看起来抽象难以理解，实际上它做了两件事，<strong>归一化</strong>和<strong>缩放与平移</strong>，首先，对当前批次（Batch）中的所有样本，计算每个通道的均值和方差。然后，用公式：<span
class="math inline">$(x - 均值) / \sqrt(方差 + \epsilon)$</span>，其中$
$是一个很小的数（如
1e-5），防止除零错误（基本操作）。最后，使用两个可学习的参数：</p>
<p><span
class="math inline"><em>γ</em></span>（缩放因子）：控制数据的缩放程度</p>
<p><span
class="math inline"><em>β</em></span>（平移因子）：控制数据的平移程度</p>
<p>公式：<span
class="math inline"><em>γ</em> * <em>归</em><em>一</em><em>化</em><em>后</em><em>的</em><em>数</em><em>据</em> + <em>β</em></span></p>
<p>这里需要指出的是，在上面的归一化公式计算过后，数据的结果会变成均值0，方差1（有兴趣的可以推导一下）</p>
<p>这里举一个非常通俗易懂的例子来解释BN层的作用：</p>
<p>如果你是大学里面的一个老师，你教的班很多同学都要挂科了，为了你们班上的及格率，你使出了你的“捞人大法”，先用<span
class="math inline">$(x - 均值) / \sqrt(方差 +
\epsilon)$</span>将分数变为平均分0，标准差为1的一串数，然后你开始设置你的<span
class="math inline"><em>γ</em></span>和<span
class="math inline"><em>β</em></span>值，如果你想平均分为70，那么你就将<span
class="math inline"><em>β</em></span>调整为70，如果你分数差距能够大一些，那么就将<span
class="math inline"><em>γ</em></span>设置得更大一些，比如1.5</p>
<h3 id="relu函数的意思">RELU函数的意思</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p><code>RELU</code>是什么意思？简单来说，<strong>ReLU 把所有负数变成
0，正数保持不变</strong>，就是一个非常简单的函数，但是它具有什么样的意义呢？两个意义：一个是引入非线性，激活函数显而易见是非线性函数，如果没有激活函数，无论神经网络有多少层，最终都等价于一个线性函数（因为线性变换的组合还是线性的）第二个，在一定程度上能够解决梯度的消失问题（这块我也没有太搞懂，查的资料上是这么讲的），这里的梯度其实就是高数里面梯度概念的衍生，梯度是用来指示神经网络应该朝哪个方向去调整的，并且调整多少。</p>
<h2 id="骨干网络">骨干网络</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNetBackbone</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, layers, in_channels=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-variable language_">self</span>.inplanes = <span class="hljs-number">64</span><br>        <span class="hljs-built_in">super</span>(ResNetBackbone, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 通过7x7卷积+BN+ReLU+最大池化，快速缩小特征图尺寸（降维）</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># layer1：通道数 64，特征图尺寸不变</span><br>        <span class="hljs-comment"># layer2：通道数 128，特征图尺寸减半（stride=2）</span><br>    	<span class="hljs-comment"># layer3：通道数 256，特征图尺寸减半</span><br>        <span class="hljs-comment"># layer4：通道数 512，特征图尺寸减半</span><br>        <span class="hljs-variable language_">self</span>.layer1 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">64</span>, layers[<span class="hljs-number">0</span>])<br>        <span class="hljs-variable language_">self</span>.layer2 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">128</span>, layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.layer3 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">256</span>, layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.layer4 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">512</span>, layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)<br>		<span class="hljs-comment"># 随着层数加深，通道数增加，特征图尺寸减小，抓住更多特征，抽离出更多关键信息</span><br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>                nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, planes, blocks, stride=<span class="hljs-number">1</span></span>):<br>        downsample = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.inplanes != planes * block.expansion:<br>            downsample = nn.Sequential(<br>                nn.Conv2d(<span class="hljs-variable language_">self</span>.inplanes, planes * block.expansion,<br>                          kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(planes * block.expansion),<br>            )<br><br>        layers = []<br>        layers.append(block(<span class="hljs-variable language_">self</span>.inplanes, planes, stride, downsample))<br>        <span class="hljs-variable language_">self</span>.inplanes = planes * block.expansion<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, blocks):<br>            layers.append(block(<span class="hljs-variable language_">self</span>.inplanes, planes))<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 通过第一个卷积层和池化层，得到初始特征</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)<br>        x = <span class="hljs-variable language_">self</span>.bn1(x)<br>        x = <span class="hljs-variable language_">self</span>.relu(x)<br>        x = <span class="hljs-variable language_">self</span>.maxpool(x)<br>	    <span class="hljs-comment"># 逐层提取特征，每层输出的特征图尺寸逐渐减小，通道数逐渐增加</span><br>        x1 = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x2 = <span class="hljs-variable language_">self</span>.layer2(x1)<br>        x3 = <span class="hljs-variable language_">self</span>.layer3(x2)<br>        x4 = <span class="hljs-variable language_">self</span>.layer4(x3)<br>	   <span class="hljs-comment"># 返回四层的特征图，用于后续任务</span><br>        <span class="hljs-keyword">return</span> [x1, x2, x3, x4]<br></code></pre></td></tr></table></figure>
<p>这一块是ResNet的核心部分，前面和后面基本上跟上一段还是很像的，我加了一些注释，应该没有问题，难点应该是在19-41行之间，但是在这之前，我们先补一个小点，就是<code>maxpool</code>函数的意思</p>
<h3 id="maxpool函数的意思">maxpool函数的意思</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>这个函数也是PyTorch里面的，意思为最大池化层，最大池化就像一个
“筛选器”，它在输入数据上滑动一个小窗口，每次只保留窗口内的<strong>最大值</strong>，其他值全部丢弃。我们的<code>kernel_size</code>就是池化窗口的大小（单位也是像素），<code>stride</code>是每次走的步长（单位是像素），<code>padding</code>在输入数据周围填充多少圈
0（这里是1圈）</p>
<p>我们在进行池化后，提取出了图像中最为主要的特征，并且减小了数据尺寸</p>
<h3 id="kaiming-初始化方法">Kaiming 初始化方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>Kaiming初始化是专门为ReLU激活函数设计的初始化方法，它的目标是：让神经网络各层的激活值和梯度的方差在传播过程中保持稳定，避免出现梯度消失或爆炸。</p>
<p>这里讲的有些抽象了，我们先来解决一下里面的参数的意义：</p>
<p><code>m.weight</code>：需要初始化的卷积层权重张量。</p>
<p><code>mode='fan_out'</code>：</p>
<ul>
<li>控制权重初始化的缩放因子。</li>
<li><code>'fan_out'</code>保持输出方差的稳定性（更适合卷积层）。</li>
</ul>
<p><code>nonlinearity='relu'</code>：</p>
<ul>
<li>指定激活函数类型为 ReLU。</li>
<li>Kaiming
初始化会根据不同的激活函数调整缩放因子（ReLU舍弃掉了负的一半区间，ReLU
需要额外除以 2）。</li>
</ul>
<p>初始化的结果是，让权重服从正态分布，标准差为<span
class="math inline">$\sqrt(\frac{2}{fanout})$</span>，<code>fan_out</code>
表示输出通道数，控制方差以保持梯度稳定。</p>
<p>如果觉得这个表述还是太抽象的话，举个最简单的例子：如果组织一场接力赛，如果我们每个选手的初始跑步速度都是随机的，那么可能会出现几个问题，前面的选手跑得太快，后面的选手接不住棒（梯度爆炸），前面的选手跑得太慢，接力棒到后面几乎停了（梯度消失），而我们的Kaiming初始化就是根据赛道长度和选手数量，精确计算每个选手的初始速度，确保接力棒能稳定、快速地传递到终点。</p>
<p>这个初始化方法基本上就是对照着ReLU激活来的，也是特别适合ResNet这种大量使用ReLU的网络。</p>
<p>那么我们终于可以开始解释了</p>
<h3 id="参数初始化操作">参数初始化操作</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<span class="hljs-comment"># 遍历模型中的每一个层（包括卷积层、BN层、ReLU等）</span><br>	<span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>		nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>	<span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>		nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>		nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p>前置的东西讲完，这个东西就很好解释了，这段代码就是神经网络里面的参数初始化操作，它会遍历模型里面的每一个层，并且根据层的类型设置不同的初始值，如果是卷积层，那么就使用Kaiming初始化，如果是BN层，就使用BatchNorm
层初始化（前文都有介绍）。其实，这些初始化，都是为了保证梯度在神经网络中传递的稳定性</p>
<h3 id="make_layer函数">_make_layer函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, planes, blocks, stride=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-comment"># block: 残差块类型（如Bottleneck）</span><br>    <span class="hljs-comment"># planes: 基础通道数（如64、128等）</span><br>    <span class="hljs-comment"># blocks: 该层包含的残差块数量</span><br>    <span class="hljs-comment"># stride: 步长，控制特征图尺寸是否减半（默认为1）</span><br>    downsample = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.inplanes != planes * block.expansion:<br>        downsample = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-variable language_">self</span>.inplanes, planes * block.expansion,<br>                      kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(planes * block.expansion),<br>        )<br><br>    layers = []<br>    layers.append(block(<span class="hljs-variable language_">self</span>.inplanes, planes, stride, downsample))<br>    <span class="hljs-variable language_">self</span>.inplanes = planes * block.expansion<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, blocks):<br>        layers.append(block(<span class="hljs-variable language_">self</span>.inplanes, planes))<br><br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure>
<p>我把参数的意义写在备注上，接下来就是解释了，这一段是定义ResNet中的“残差层”的核心逻辑，就是通过堆叠多个残差块来组成网络的一层，这是一个大层，而这个大层的特点就是第一层可能包括下采样（downsample）操作，用于调整特征图的尺寸和通道数，后续层保持相同的通道数和特征图尺寸，这里我再解释一下什么是下采样，下采样可以简单的理解为“缩小图片”的操作，目的是让图片变小同时保留重要信息，至于下采样的方式，就是可以用之前的“池化层”，每隔两个像素来取一个最大值，图片直接编程之前的一半，或者使用1x1的卷积层，通过调整步长，比如stride=2，让输出的图片尺寸缩小。</p>
<p>而我们这边需要判断是否要进行下采样，主要是有两种情况：</p>
<p><code>stride != 1</code>：需要调整特征图尺寸（通常为 2，即减半）</p>
<p><code>self.inplanes != planes * block.expansion</code>：输入通道数与输出通道数不匹配</p>
<p>这边下采样的实现就是用1x1卷积来调整通道参数</p>
<p>下采样结束之后，其实后续就没什么特殊的了，就是保持相同的通道数和特征图尺寸，输入通道数已经在第一层后更新为<code>planes * block.expansion</code>了</p>
<p>假设我们要构建 ResNet 的第二层（layer2）：</p>
<ul>
<li><code>block</code> = Bottleneck（瓶颈块，expansion=4）</li>
<li><code>planes</code> = 128</li>
<li><code>blocks</code> = 4（该层有 4 个瓶颈块）</li>
<li><code>stride</code> = 2（需要下采样，特征图尺寸减半）</li>
</ul>
<p>第一个瓶颈块： - 输入通道数：256（来自 layer1 的输出） -
中间通道数：128 - 输出通道数：128×4 = 512 - 包含下采样：1x1
卷积（256→512，stride=2） - 特征图尺寸：从 56x56→28x28</p>
<p>后续三个瓶颈块： - 输入 / 输出通道数：512→512 - 中间通道数：128 -
特征图尺寸保持 28x28 不变</p>
<p>这样子基本上我们的骨干网络就全部讲清楚了，可以不断地往下传。</p>
<h2 id="特征金字塔网络">特征金字塔网络</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeaturePyramidNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels_list, out_channels</span>):<br>        <span class="hljs-built_in">super</span>(FeaturePyramidNetwork, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.inner_blocks = nn.ModuleList()<br>        <span class="hljs-variable language_">self</span>.layer_blocks = nn.ModuleList()<br>        <span class="hljs-keyword">for</span> in_channels <span class="hljs-keyword">in</span> in_channels_list:<br>            <span class="hljs-keyword">if</span> in_channels == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-comment"># 1x1 卷积，用于调整不同层级特征的通道数（统一为out_channels）</span><br>            inner_block = nn.Conv2d(in_channels, out_channels, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 3x3 卷积，用于处理融合后的特征</span><br>            layer_block = nn.Conv2d(out_channels, out_channels, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>            <span class="hljs-variable language_">self</span>.inner_blocks.append(inner_block)<br>            <span class="hljs-variable language_">self</span>.layer_blocks.append(layer_block)<br>			<span class="hljs-comment"># 这里还是对每一层进行BN初始化</span><br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_uniform_(m.weight, a=<span class="hljs-number">1</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        last_inner = <span class="hljs-variable language_">self</span>.inner_blocks[-<span class="hljs-number">1</span>](x[-<span class="hljs-number">1</span>])<br>        results = []<br>        results.append(<span class="hljs-variable language_">self</span>.layer_blocks[-<span class="hljs-number">1</span>](last_inner))<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x) - <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>            inner_lateral = <span class="hljs-variable language_">self</span>.inner_blocks[i](x[i])<br>            feat_shape = inner_lateral.shape[-<span class="hljs-number">2</span>:]<br>            last_inner = F.interpolate(last_inner, size=feat_shape, mode=<span class="hljs-string">&quot;nearest&quot;</span>)<br>            last_inner = last_inner + inner_lateral<br>            results.insert(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.layer_blocks[i](last_inner))<br><br>        <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure>
<p>在上面一个篇章，我们已经处理好了很多个大层，这里面的特征金字塔网络（<strong>Feature
Pyramid Network,
FPN</strong>），目标是融合不同层级的特征，就是把我们刚刚得到的诸多大层的特征再次融合在一起，使得我们的提取结果既有高度概括的特征，又有细致的细节，就是相当于“显微镜+望远镜”，既能看清细节，又能把握全局</p>
<p>前半部分的初始化过程我就不多说了，跟前面的内容有很多重合的，后面的<code>forward</code>函数还是有些意思的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># 处理最深层特征（最高语义，最小尺寸）</span><br>    last_inner = <span class="hljs-variable language_">self</span>.inner_blocks[-<span class="hljs-number">1</span>](x[-<span class="hljs-number">1</span>])<br>    results = [<span class="hljs-variable language_">self</span>.layer_blocks[-<span class="hljs-number">1</span>](last_inner)]<br>    <span class="hljs-comment"># 自顶向下融合过程</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x) - <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># 获取当前层特征并调整通道</span><br>        inner_lateral = <span class="hljs-variable language_">self</span>.inner_blocks[i](x[i])<br>        <span class="hljs-comment"># 上采样高层特征，使其与当前层尺寸匹配</span><br>        last_inner = F.interpolate(last_inner, size=inner_lateral.shape[-<span class="hljs-number">2</span>:], mode=<span class="hljs-string">&quot;nearest&quot;</span>)<br>        <span class="hljs-comment"># 特征融合：将上采样后的高层特征与当前层特征相加</span><br>        last_inner = last_inner + inner_lateral<br>        <span class="hljs-comment"># 处理融合后的特征并保存结果</span><br>        results.insert(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.layer_blocks[i](last_inner))<br>    <span class="hljs-keyword">return</span> results<br></code></pre></td></tr></table></figure>
<p>我本来还是想继续写一些东西的，后来觉得代码注释已经把我想写的全部写完了，可能还是会觉得抽象，来举个例子吧：</p>
<p>假设输入是来自ResNet的四层特征:</p>
<blockquote>
<p>C2: 256通道, 尺寸56×56（最浅层）</p>
<p>C3: 512通道, 尺寸28×28</p>
<p>C4: 1024通道, 尺寸14×14</p>
<p>C5: 2048通道, 尺寸7×7（最深层）</p>
</blockquote>
<p>用 1x1 卷积将
C2→256，C3→256，C4→256，C5→256（假设<code>out_channels=256</code>）</p>
<p>自顶向下融合：</p>
<blockquote>
<p>P5 = 调整后的C5（尺寸7×7）</p>
<p>P4 = 调整后的C4 + 上采样(P5)（尺寸14×14）</p>
<p>P3 = 调整后的C3 + 上采样(P4)（尺寸28×28）</p>
<p>P2 = 调整后的C2 + 上采样(P3)（尺寸56×56）</p>
</blockquote>
<p>最终输出：</p>
<blockquote>
<p>[P2, P3, P4, P5]（通道数均为256，尺寸从大到小）</p>
</blockquote>
<p>大尺寸特征图（如 P2）适合检测小物体（保留了细节）</p>
<p>小尺寸特征图（如 P5）适合检测大物体（包含了高层语义）</p>
<p>基本上我们这个FPN也讲清楚了</p>
<h2 id="多尺度注意力特征">多尺度注意力特征</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MAF_Extractor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feat_dim=<span class="hljs-number">2048</span>, maf_dim=<span class="hljs-number">256</span></span>):<br>        <span class="hljs-built_in">super</span>(MAF_Extractor, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.global_avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<span class="hljs-comment"># 全局平均池化</span><br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(feat_dim, maf_dim)<span class="hljs-comment"># 全连接层，降维</span><br>        <span class="hljs-variable language_">self</span>.bn = nn.BatchNorm1d(maf_dim)<span class="hljs-comment"># 批量归一化</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 激活函数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.global_avgpool(x)<span class="hljs-comment"># 压缩特征图为1×1</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<span class="hljs-comment"># 展平为一维向量</span><br>        x = <span class="hljs-variable language_">self</span>.fc(x)<span class="hljs-comment"># 降维：feat_dim → maf_dim</span><br>        x = <span class="hljs-variable language_">self</span>.bn(x)<span class="hljs-comment"># 归一化</span><br>        x = <span class="hljs-variable language_">self</span>.relu(x)<span class="hljs-comment"># 激活函数</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>这段定义了一个<strong>MAF_Extractor</strong>（Multi-scale Attention
Feature, MAF）的神经网络模块，如果简单解释MAF_Extractor 就像一个
“特征压缩器”，它接收高维的图像特征（例如 ResNet 输出的 2048
维特征），通过一系列操作将其转换为低维但更有代表性的特征（例如 256
维）。</p>
<p>里面的函数也非常好理解，很多是之前已经出现过的</p>
<h3 id="adaptiveavgpool2d函数">AdaptiveAvgPool2d函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.global_avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure>
<p>假设输入是一个<code>C×H×W</code>的特征图（C 是通道数，H 和 W
是高度和宽度），全局平均池化会：</p>
<ol type="1">
<li>对每个通道（channel）单独处理</li>
<li>计算每个通道上所有像素值的平均值</li>
<li>输出一个<code>C×1×1</code>的向量，每个值对应一个通道的平均值</li>
</ol>
<p>这个简单到不需要举例子了</p>
<p>参数就是矩阵的大小，因为这里输入的是<code>(1,1)</code>，如果两个参数都设成2，那么就会使<code>2x2</code>的矩阵</p>
<h3 id="linear函数">Linear函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.fc = nn.Linear(feat_dim, maf_dim)<br></code></pre></td></tr></table></figure>
<p>全连接层就像一个 “翻译器”，把输入的特征向量（比如 2048 维）“翻译”
成另一个维度的向量（比如 256 维）。这个过程可以用一个线性方程表示：</p>
<blockquote>
<p><code>y = Wx + b</code></p>
</blockquote>
<p><code>x</code> 是输入向量（维度：<code>feat_dim</code>）</p>
<p><code>W</code>
是权重矩阵（维度：<code>maf_dim × feat_dim</code>）</p>
<p><code>b</code> 是偏置向量（维度：<code>maf_dim</code>）</p>
<p><code>y</code> 是输出向量（维度：<code>maf_dim</code>）</p>
<p>全连接层的参数就是<code>W</code>和<code>b</code>，这个数值需要通过训练来得到</p>
<p><code>feat_dim</code>：输入特征的维度（这里是2048，来自上一层的输出）</p>
<p><code>maf_dim</code>：输出特征的维度（这里是256，压缩后的特征维度）</p>
<p>我们需要特别注意的是，这个函数既可以用来降维，也可以用来升维，后面会多次遇到</p>
<h2 id="姿势回归器">姿势回归器</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PoseRegressor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feat_dim=<span class="hljs-number">256</span>, output_dim=<span class="hljs-number">24</span>*<span class="hljs-number">3</span>*<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>(PoseRegressor, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(feat_dim, <span class="hljs-number">1024</span>)<span class="hljs-comment"># 第一个全连接层：升维</span><br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 第一个Dropout层</span><br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>)<span class="hljs-comment"># 第二个全连接层</span><br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 第二个Dropout层</span><br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">1024</span>, output_dim)<span class="hljs-comment"># 第三个全连接层：输出结果</span><br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<span class="hljs-comment"># ReLU激活函数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.dropout1(<span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.fc1(x)))<span class="hljs-comment"># 第一层：全连接+激活+Dropout</span><br>        x = <span class="hljs-variable language_">self</span>.dropout2(<span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.fc2(x)))<span class="hljs-comment"># 第二层：全连接+激活+Dropout</span><br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<span class="hljs-comment"># 第三层：直接输出结果</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>这段代码定义了一个<strong>姿态回归器（PoseRegressor）</strong>，它的作用是从输入特征中预测人体的
3D 姿态。姿态回归器就像一个
“姿势解码器”，它接收一个特征向量（MAF_Extractor 输出的 256
维特征），然后预测出人体各个关节的 3D 位置。</p>
<p>这里面的<code>Dropout</code>让我自己想肯定是想不到的，给的说法是随机丢掉50%的神经元，防止过拟合，这可能是在实践中发现存在错误后而进行的修正吧</p>
<p>它的特点就是逐步处理特征，从 256 维→1024 维→1024 维→216 维（216 = 24
x 3 x
3）这个数字代表的就是24个关节点，每个关节点用3*3的旋转矩阵来表示，就是SMPL的核心参量了</p>
<p>为什么要升维？从 256 维升到 1024 维，让模型有更多 “空间”
学习复杂的姿势特征但是可能也是意味着维数越多，拟合出来的就越精确吧，反正<code>Linear</code>函数里面的<code>W</code>和<code>b</code>都是训练出来的，只要给合适的数据就行。</p>
<h2 id="形状回归器">形状回归器</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShapeRegressor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feat_dim=<span class="hljs-number">256</span>, output_dim=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(ShapeRegressor, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(feat_dim, <span class="hljs-number">1024</span>)<br>        <span class="hljs-variable language_">self</span>.dropout1 = nn.Dropout(<span class="hljs-number">0.5</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>)<br>        <span class="hljs-variable language_">self</span>.dropout2 = nn.Dropout(<span class="hljs-number">0.5</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">1024</span>, output_dim)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.dropout1(<span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.fc1(x)))<br>        x = <span class="hljs-variable language_">self</span>.dropout2(<span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.fc2(x)))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>这里面的代码几乎完全相同，唯一不同的是最终输出维度是10维，是是因为我们的形状参数就是10维的（后来更新到50维，但是现在开源的还是只有10维），还是那句话，<code>Linear</code>函数里面的<code>W</code>和<code>b</code>都是训练出来的，所以这个模板在姿态和形状上面都可以使用。</p>
<h2 id="相机回归器">相机回归器</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CamRegressor</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feat_dim=<span class="hljs-number">256</span></span>):<br>        <span class="hljs-built_in">super</span>(CamRegressor, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(feat_dim, <span class="hljs-number">3</span>)<span class="hljs-comment"># 全连接层：256维→3维</span><br>        nn.init.zeros_(<span class="hljs-variable language_">self</span>.fc.weight)<span class="hljs-comment"># 权重初始化为0</span><br>        nn.init.ones_(<span class="hljs-variable language_">self</span>.fc.bias)<span class="hljs-comment"># 偏置初始化为1</span><br>        nn.init.constant_(<span class="hljs-variable language_">self</span>.fc.bias[<span class="hljs-number">0</span>], <span class="hljs-number">0.9</span>)<span class="hljs-comment"># 第一个偏置设为0.9</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.fc(x)<br></code></pre></td></tr></table></figure>
<p>这段代码更简单，就是从输入特征中与预测相机参数，还是那话，由于<code>Linear</code>函数的存在，初始值不必太在意，我们这边需要关注一下它的输出的三位向量分别代表什么意思</p>
<ol type="1">
<li>缩放因子（scale）：控制人体在图像中的大小，值越大，人体看起来越大（相机离人体越近）</li>
<li>水平平移（tx）：控制人体在图像中的水平位置</li>
<li>垂直平移（ty）：控制人体在图像中的垂直位置</li>
</ol>
<p>这些参数可以将 3D 人体模型投影到 2D
图像平面上，使其与输入图像中的人体对齐。</p>
<h2 id="pymaf-x模型核心">PyMAF-X模型（核心）</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PyMAF_X</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, pretrained=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(PyMAF_X, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.backbone = ResNetBackbone(Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<span class="hljs-comment"># 特征提取骨干网络</span><br>        <span class="hljs-variable language_">self</span>.fpn = FeaturePyramidNetwork([<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">2048</span>], <span class="hljs-number">256</span>)<span class="hljs-comment"># 特征融合</span><br>        <span class="hljs-variable language_">self</span>.maf_extractor = MAF_Extractor(<span class="hljs-number">2048</span>, <span class="hljs-number">256</span>)<span class="hljs-comment"># 多尺度注意力特征提取</span><br>        <span class="hljs-variable language_">self</span>.pose_regressor = PoseRegressor(<span class="hljs-number">256</span>, <span class="hljs-number">24</span>*<span class="hljs-number">3</span>*<span class="hljs-number">3</span>)<span class="hljs-comment"># 姿态回归</span><br>        <span class="hljs-variable language_">self</span>.shape_regressor = ShapeRegressor(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<span class="hljs-comment"># 形状回归</span><br>        <span class="hljs-variable language_">self</span>.cam_regressor = CamRegressor(<span class="hljs-number">256</span>)<span class="hljs-comment"># 相机参数回归</span><br><br>        <span class="hljs-keyword">if</span> pretrained:<br>            <span class="hljs-variable language_">self</span>._load_pretrained_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_load_pretrained_weights</span>(<span class="hljs-params">self</span>):<br>        resnet = resnet50(pretrained=<span class="hljs-literal">True</span>)<br>        backbone_state_dict = <span class="hljs-variable language_">self</span>.backbone.state_dict()<br>        resnet_state_dict = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> resnet.state_dict().items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> backbone_state_dict&#125;<br>        backbone_state_dict.update(resnet_state_dict)<br>        <span class="hljs-variable language_">self</span>.backbone.load_state_dict(backbone_state_dict)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        features = <span class="hljs-variable language_">self</span>.backbone(x)<span class="hljs-comment"># 提取多尺度特征</span><br>        fpn_features = <span class="hljs-variable language_">self</span>.fpn(features)<span class="hljs-comment"># 融合特征</span><br>        global_feat = <span class="hljs-variable language_">self</span>.maf_extractor(features[-<span class="hljs-number">1</span>])<span class="hljs-comment"># 提取全局特征</span><br>        <br>        pose = <span class="hljs-variable language_">self</span>.pose_regressor(global_feat)<span class="hljs-comment"># 预测姿态</span><br>        shape = <span class="hljs-variable language_">self</span>.shape_regressor(global_feat)<span class="hljs-comment"># 预测形状</span><br>        cam = <span class="hljs-variable language_">self</span>.cam_regressor(global_feat)<span class="hljs-comment"># 预测相机参数</span><br>        <br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&#x27;pose&#x27;</span>: pose,<br>            <span class="hljs-string">&#x27;shape&#x27;</span>: shape,<br>            <span class="hljs-string">&#x27;cam&#x27;</span>: cam<br>        &#125;<br></code></pre></td></tr></table></figure>
<p>铺垫了这么久，终于千呼万唤始出来了，除了其中的预训练权重，其他的基本上是所见即所得了。预训练就是将已经训练好的ResNet50加载下来，将这些知识迁移到PyMAF-X中，站在巨人的肩膀上，大大减少了运算量。</p>
<p>虽然非常简单，我还是想在这边做一个总结，我们的处理流程</p>
<blockquote>
<p>输入图像 → ResNet骨干网络 → 特征金字塔融合 → 全局特征提取 →</p>
<p>→ 姿态回归器 → 姿态参数</p>
<p>→ 形状回归器 → 形状参数</p>
<p>→ 相机回归器 → 相机参数</p>
</blockquote>
<h1 id="smpl的实现">SMPL的实现</h1>
<p>SMPL（Skinned Multi-Person Linear
Model）是一种裸体的（skinned），基于顶点（vertex-based）的人体三维模型，能够精确地表示人体的不同形状（shape）和姿态（pose）。</p>
<h2 id="前置概念">前置概念</h2>
<p>在进入SMPL模型之前，需要先明确2个概念：</p>
<p>1.顶点（vertex）：小三角形，看作一个顶点（记为N，在人体中应该是有6890个）</p>
<p>2.骨骼点：关节点，姿态估计的关键点（记为K，是23个三维旋转向量）</p>
<h2 id="理论推导">理论推导</h2>
<p>我们先来一些模型参数<span
class="math inline"><em>Φ</em> = {<em>T</em>, <em>W</em>, <em>S</em>, <em>J</em>, <em>P</em>}</span>，这些参数不是输入量，这些参数是通过训练得到的，我们的输入参数是<span
class="math inline">$\vecβ,\vecθ,\vecθ^*$</span></p>
<p>我们来逐个解释其中的含义：</p>
<p><span
class="math inline">$\vecβ=[\vecβ_1,…,\vecβ_{∣β∣}]^T$</span>：形状参数，我们上面已经通过2D单目图像得到了</p>
<p><span
class="math inline">$\vecθ=[\vecω^T_0,…,\vecω^T_K]^T$</span>：姿态参数，<span
class="math inline"><em>w</em><sub><em>k</em></sub></span>指关节k相对于运动树（kinematic
tree）中的父关节点的旋转轴角度，<span
class="math inline"><em>ω</em><sub><em>k</em></sub> ∈ <em>R</em><sup>3</sup></span>，我们上面也已经得到了</p>
<p><span
class="math inline"><em>W</em> ∈ <em>R</em><sup><em>N</em> × <em>K</em></sup></span>：一组混合权重，BS/QBS混合权重矩阵，即关节点对顶点的影响权重
(第几个顶点受哪些关节点的影响且权重分别为多少)</p>
<p><span
class="math inline">$S=[S_1,…,S_{|\vecβ|}]∈R^{3N×|\vecβ|}$</span>：由<span
class="math inline">$\vecβ$</span>带来的顶点位置修正</p>
<p><span
class="math inline"><em>P</em> = [<em>P</em><sub>1</sub>, …, <em>P</em><sub><em>k</em></sub>] ∈ <em>R</em><sup>3<em>N</em> × <em>k</em></sup></span>：由<span
class="math inline"><em>θ⃗</em></span>带来的顶点位置修正</p>
<p><span class="math inline">$\overline
T∈R^{3N}$</span>：T姿态，作为平均模型，后面的修改都是建立在它的基础上的</p>
<p>J: 将rest vertices转换成rest joints的矩阵（获取T
pose的关节点坐标的矩阵）[完成顶点到关节的转化]</p>
<p>对于SMPL模型来说，我们主要分为几个步骤：</p>
<blockquote>
<p>1.将shape缩放</p>
<p>2.根据shape调整joint</p>
<p>3.调整胖瘦变形</p>
<p>4.确定姿势</p>
<p>5.给骨架包裹外衣，蒙皮</p>
</blockquote>
<p>SMPL 10个shape的意义分别对应的是：</p>
<blockquote>
<p>0
代表整个人体的胖瘦和大小，初始为0的情况下，正数变瘦小，负数变大胖（±5）</p>
<p>1 侧面压缩拉伸，正数压缩</p>
<p>2 正数变胖大</p>
<p>3 负数肚子变大很多，人体缩小</p>
<p>4 代表
chest、hip、abdomen的大小，初始为0的情况下，正数变大，负数变小（±5）</p>
<p>5 负数表示大肚子+整体变瘦</p>
<p>6 正数表示肚子变得特别大的情况下，其他部位非常瘦小</p>
<p>7 正数表示身体被纵向挤压</p>
<p>8 正数表示横向表胖</p>
<p>9 正数表示肩膀变宽</p>
</blockquote>
<p><span
class="math inline">$M(\vecβ,\vecθ;Φ)=W(T_P(\vecβ,\vecθ),J(\vecβ),\vecθ,W)R^{∣\vecθ∣×∣\vecβ∣}↦R^{3N}$</span>：将形状和位姿参数映射到顶点</p>
<p><span class="math inline">$W(\overline
T,J,\vecθ,W):R^{3N×3K×|\vecθ|×|W|}↦R^{3N}$</span>:标准线性混合蒙皮.</p>
<p><span
class="math inline">$B_P(\vecθ):R^{|θ|}↦R^{3N}$</span>：输入是一系列姿势参数向量，代表姿势的相关形变引发的顶点的修正</p>
<p><span
class="math inline"><em>B</em><sub><em>S</em></sub>(<em>β⃗</em>) : <em>R</em><sup>|<em>β</em>|</sup> ↦ <em>R</em><sup>3<em>N</em></sup></span>：输入是一系列姿势参数向量，代表姿势的相关形变引发的顶点的修正</p>
<p><span class="math inline">$J(\vecβ):R^{∣β∣}↦R^{3K}$</span>:
一个预测K个关节位置的函数.</p>
<p>每个关节<span
class="math inline"><em>j</em></span>绕轴的旋转角用罗德里格斯公式转换成旋转矩阵：</p>
<p><span class="math inline">$exp(\vec ω_j)=I+\hat{\overline
ω_j}sin(∥\vec ω_j∥)+\hat ω^2_jcos(∥\vecωj∥)$</span></p>
<p>其中，<span
class="math inline">$\vecθ=[\vecω^T_0,…,\vecω^T_K]^T$</span>，参数通过<span
class="math inline">$|\vecθ|=3×23+3=72$</span>定义</p>
<p><span class="math inline">$\overline
ω=\frac{\vecω}{|∣ω|∣}$</span>:为旋转的单位轴，单位化了</p>
<p><span
class="math inline">$\hatω$</span>：斜对称矩阵，通过三维向量<span
class="math inline">$\overline ω$</span>组成</p>
<p><span class="math inline"><em>I</em>：3 × 3</span>单位矩阵</p>
<p>下面就是这个函数最为神奇的理论推导的地方（公式太难用latex打出来了，这里我就放一张图片）：</p>
<p><img src="/picture/SMPL1.jpg" srcset="/img/loading.gif" lazyload /></p>
<p>其中，<span
class="math inline"><em>ω</em><sub><em>k</em>, <em>i</em></sub></span>是混合权重矩阵<span
class="math inline"><em>W</em></span>的元素，代表第<span
class="math inline"><em>k</em></span>部分的旋转角度有多少程度影响了第<span
class="math inline"><em>i</em></span>个顶点。</p>
<p><span class="math inline">$exp(\vecθ_j)$</span>为局部<span
class="math inline">3 × 3</span>旋转矩阵，对应结点<span
class="math inline"><em>j</em></span>。</p>
<p><span class="math inline">$G_k(\vecθ,J)$</span> 是关节<span
class="math inline"><em>k</em></span>的世界变换</p>
<p><span
class="math inline">$G^′_k(\vecθ,J)$</span>是移除了变换后的相同变换，相当于是在确定你的坐标系</p>
<p><span class="math inline"><em>J</em></span>:关节回归函数。</p>
<p>我们假设W是稀疏的，最多允许四个部分影响一个顶点，那这样子我们对我们的公式还可以进行一定的补充：
<span class="math display">$$
M(\vecβ,\vecθ;Φ)=W(T_P(\vecβ,\vecθ),J(\vecβ),\vecθ,W)\\
T_p(\vecβ,\vecθ)=\overline T+B_s(\vecβ)+B_p(\vecθ)
$$</span> <span
class="math inline">$B_S(\vecβ),B_P(\vecθ)$</span>表示由shape和pose引起的相对于SMPL标准模板的顶点向量<span
class="math inline">$\overline t_i$</span>的偏移量 <span
class="math display">$$
\overline t'_i=\sum_{k=1}^{K}w_{k,i}G^′_k(\vecθ,J(\vec\beta))(\overline
t_i+b_{s,i}(\vec\beta)+b_{P,i}(\vec\theta))
$$</span> 其中，<span class="math inline">$b_{S,i}(\vecβ),b_{P,i}(\vecθ)
分别B_S(\vecβ),B_P(\vec θ)$</span>的顶点，表示相对于顶点<span
class="math inline">$\overline t_i$</span>的偏移量。 <span
class="math display">$$
B_S(\vecβ;S)=\sum^\vec{|β|}_{n=1}β_nS_n
$$</span></p>
<p><span
class="math inline">$\vecβ=[β_1,…,β_{|\vecβ|}]^T，|\vecβ|$</span>是线性形状系数的数量。</p>
<p><span
class="math inline"><em>S</em><sub><em>n</em></sub> ∈ <em>R</em><sup>3<em>N</em></sup></span>：
形状位移的标准正交主分量</p>
<p><span
class="math inline">$S=[S_1,…,S_{|\vecβ|}]∈R^{3N×|\vecβ|}$</span>为形状位移矩阵。线性函数<span
class="math inline">$B_S(\vecβ;S)$</span>能够完全被矩阵<span
class="math inline"><em>S</em></span>定义,通过注册训练网络学习。</p>
<p>定义R：<span
class="math inline">$R^{|\vecθ|}↦R^{9K}$</span>为把一个位姿向量映射到连接部分相对旋转矩阵的向量上<span
class="math inline"><em>θ⃗</em></span>,由于我们的骨骼节点有23个关节，则<span
class="math inline">$R(\vecθ)$</span>是一个<span
class="math inline">23 × 9 = 207</span>维的向量。它的元素是关节旋转角的sin和cos函数，因此它是一个对于<span
class="math inline"><em>θ⃗</em></span>的非线性函数。</p>
<p>但是作者又定义了一个可以让pose blend shape线性的函数：<span
class="math inline">$R^∗(\vecθ)=(R(\vecθ)−R(\vecθ^∗))$</span>,其中，→θ∗定义了rest
pose. 定义<span class="math inline">$R_n(\vecθ)$</span>为<span
class="math inline">$R(\vecθ)$</span>的第n个向量，则与静止模板的偏差为：
<span class="math display">$$
B_p(\vec \theta;P)=\sum^{9K}_{n=1}(R_n(\vec
\theta)-R_n(\vec\theta^*))P_n
$$</span> 其中，<span
class="math inline"><em>P</em><sub><em>n</em></sub> ∈ <em>R</em><sup>3<em>N</em></sup></span>表示顶点偏移的向量。</p>
<p><span
class="math inline"><em>P</em> = [<em>P</em><sub>1</sub>, …, <em>P</em><sub>9<em>K</em></sub>] ∈ <em>R</em><sup>3<em>N</em> × 9<em>K</em></sup></span>是所有207个pose
blend shape组成的矩阵。<span
class="math inline">$B_P(\vecθ)$</span>完全被矩阵P定义。</p>
<p>不同的体型有不同的关节位置，每个关节由其在静止位姿（rest
pose）中的3D位置表示。关节3D位置相对于身体形状的函数如下： <span
class="math display">$$
J(\vecβ;J,\overline T,S)=J(\overline T+B_S(\vecβ;S))
$$</span> 其中，J是将rest vertices转换成rest
joints的矩阵，我们从不同的人在不同的姿势的例子中学习回归矩阵J。</p>
<p>SMPL最后被定义为： <span class="math display">$$
M(\vecβ,\vecθ;Φ)=W(T_P(\vecβ,\vecθ;\overline T,S,P),J(\vecβ;J,\overline
T,S),\vecθ,W)
$$</span> 每个顶点变为： <span class="math display">$$
t_i’=\sum_{k=1}^Kw_{k,i}G'_k(\vec\theta,J(\vec\beta;J,\overline
T,S))t_{P,i}(\vec\beta,\vec\theta;\overline T,S,P)
$$</span> 其中 <span class="math display">$$
t_{P,i}(\vec\beta,\vec\theta;\overline T,S,P)=\overline
t_i+\sum_{m=1}^{|\vec\beta|}\beta_ms_{m,i}+\sum^{9K}_{n=1}(R_n(\vec
\theta)-R_n(\vec\theta^*))p_{n,i}
$$</span> 理论推导到这边就基本上完整了</p>
<h2 id="实现代码">实现代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SMPL</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path</span>):<br>        <span class="hljs-comment"># 加载模型参数</span><br>        params = np.load(model_path)<br>        <span class="hljs-variable language_">self</span>.v_template = params[<span class="hljs-string">&#x27;v_template&#x27;</span>]  <span class="hljs-comment"># 模板网格</span><br>        <span class="hljs-variable language_">self</span>.shapedirs = params[<span class="hljs-string">&#x27;shapedirs&#x27;</span>]    <span class="hljs-comment"># 形状主成分</span><br>        <span class="hljs-variable language_">self</span>.posedirs = params[<span class="hljs-string">&#x27;posedirs&#x27;</span>]      <span class="hljs-comment"># 姿态主成分</span><br>        <span class="hljs-variable language_">self</span>.J_regressor = params[<span class="hljs-string">&#x27;J_regressor&#x27;</span>] <span class="hljs-comment"># 关节回归器</span><br>        <span class="hljs-variable language_">self</span>.weights = params[<span class="hljs-string">&#x27;weights&#x27;</span>]        <span class="hljs-comment"># 蒙皮权重</span><br>        <span class="hljs-variable language_">self</span>.kintree_table = params[<span class="hljs-string">&#x27;kintree_table&#x27;</span>] <span class="hljs-comment"># 骨骼树结构</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, beta, theta, get_skin=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-comment"># 计算形状变形</span><br>        v_shaped = <span class="hljs-variable language_">self</span>.v_template + np.dot(<span class="hljs-variable language_">self</span>.shapedirs, beta)<br>        <br>        <span class="hljs-comment"># 计算关节位置</span><br>        J = np.dot(<span class="hljs-variable language_">self</span>.J_regressor, v_shaped)<br>        <br>        <span class="hljs-comment"># 计算姿态旋转矩阵</span><br>        Rs = <span class="hljs-variable language_">self</span>.rodrigues(theta[<span class="hljs-number">3</span>:].reshape((-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)))<br>        pose_feature = (Rs - np.eye(<span class="hljs-number">3</span>)).ravel()<br>        <br>        <span class="hljs-comment"># 计算姿态变形</span><br>        v_posed = v_shaped + np.dot(<span class="hljs-variable language_">self</span>.posedirs, pose_feature)<br>        <br>        <span class="hljs-comment"># 构建全局变换矩阵</span><br>        G = <span class="hljs-variable language_">self</span>.with_zeros(np.hstack((Rs, J.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>))))<br>        parent = &#123;i: <span class="hljs-variable language_">self</span>.kintree_table[<span class="hljs-number">0</span>, i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.kintree_table.shape[<span class="hljs-number">1</span>])&#125;<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, G.shape[<span class="hljs-number">0</span>]):<br>            G[i] = np.dot(G[i], np.linalg.inv(G[parent[i]]))<br>        <br>        <span class="hljs-comment"># 蒙皮过程</span><br>        T = np.tensordot(<span class="hljs-variable language_">self</span>.weights, G, axes=[[<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]])<br>        rest_shape_h = np.hstack((v_posed, np.ones((v_posed.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))))<br>        v = np.<span class="hljs-built_in">sum</span>(T * rest_shape_h.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>), axis=<span class="hljs-number">2</span>)[:, :<span class="hljs-number">3</span>]<br>        <br>        <span class="hljs-keyword">return</span> (v, J) <span class="hljs-keyword">if</span> get_skin <span class="hljs-keyword">else</span> J<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">rodrigues</span>(<span class="hljs-params">self, r</span>):<br>        <span class="hljs-comment"># 罗德里格斯公式：将旋转向量转换为旋转矩阵</span><br>        theta = np.linalg.norm(r, axis=<span class="hljs-number">1</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        r = r / (theta + <span class="hljs-number">1e-8</span>)<br>        r = r.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>        cost = np.cos(theta)<br>        sint = np.sin(theta)<br>        rx = np.array([[<span class="hljs-number">0</span>, -r[:, <span class="hljs-number">2</span>], r[:, <span class="hljs-number">1</span>]],<br>                       [r[:, <span class="hljs-number">2</span>], <span class="hljs-number">0</span>, -r[:, <span class="hljs-number">0</span>]],<br>                       [-r[:, <span class="hljs-number">1</span>], r[:, <span class="hljs-number">0</span>], <span class="hljs-number">0</span>]]).transpose(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        R = np.eye(<span class="hljs-number">3</span>) + sint * rx + (<span class="hljs-number">1</span> - cost) * np.matmul(rx, rx)<br>        <span class="hljs-keyword">return</span> R<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">with_zeros</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 在变换矩阵底部添加一行 [0, 0, 0, 1]</span><br>        <span class="hljs-keyword">return</span> np.vstack((x, np.array([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>]])))<br></code></pre></td></tr></table></figure>
<p>有了上面的推导，代码基本上就是所见即所得，上面也有一些注释，很好理解</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/SMPL%E7%B3%BB%E5%88%97/" class="category-chain-item">SMPL系列</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/SMPL/" class="print-no-link">#SMPL</a>
      
        <a href="/tags/PxMAF-X/" class="print-no-link">#PxMAF-X</a>
      
        <a href="/tags/3D%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8B%E6%81%A2%E5%A4%8D/" class="print-no-link">#3D人体模型恢复</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>PxMAF-X学习</div>
      <div>http://example.com/2025/07/08/PxMAF-X学习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>牧丛</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年7月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/07/13/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/" title="深度学习笔记1">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深度学习笔记1</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/07/04/%E7%BA%BF%E6%80%A7%E8%A1%A8%E6%95%B4%E7%90%86/" title="线性表整理">
                        <span class="hidden-mobile">线性表整理</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
